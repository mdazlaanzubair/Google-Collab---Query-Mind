{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM7NklRWSsYhZYP5L1reNa1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdazlaanzubair/Google-Collab---Query-Mind/blob/main/Query_Mind.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1 : Installing Dependencies**\n",
        "\n",
        "* Installing [**Transformers**](https://huggingface.co/docs/transformers/index) \n",
        "(a tool or platform) provided by [**Huggingface**](https://huggingface.co/). \n",
        "\n",
        "* It allows the community with APIs to access and use state-of-the-art [**pre-trained models**](https://huggingface.co/models) available from the [**Hugging Face hub**](https://huggingface.co/docs/hub/index).\n",
        "\n"
      ],
      "metadata": {
        "id": "VJpQpLAygvrG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wu-7OZWwBuo"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2 : Invoking / Importing Dependencies**"
      ],
      "metadata": {
        "id": "x1RUNtL7nCH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "xwcS3O5lmlKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3 : Loading Model**\n",
        "**Model Title** : [DistilBERT base cased distilled SQuAD](https://huggingface.co/distilbert-base-cased-distilled-squad)\n",
        "\n",
        "**Model Description** : The DistilBERT model was proposed in the blog post Smaller, faster, cheaper, lighter: Introducing DistilBERT, adistilled version of BERT, and the paper DistilBERT, adistilled version of BERT: smaller, faster, cheaper and lighter. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT's performances as measured on the GLUE language understanding benchmark."
      ],
      "metadata": {
        "id": "zn4byRiUjxxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_answerer = pipeline(\"question-answering\", model='distilbert-base-cased-distilled-squad')"
      ],
      "metadata": {
        "id": "iNUcnCyvoOrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4 : Provide context to the model**\n",
        "\n",
        "\n",
        "*   In **`context`** variable just paste the paragraph you want to explore.\n",
        "*   In **`query`** variable just type the question you want to find from the paragraph."
      ],
      "metadata": {
        "id": "k3TPOPbXr2zP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context = r\"\"\"\n",
        "Extractive Question Answering is the task of extracting an answer from a text given a question. An example     of a\n",
        "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
        "a model on a SQuAD task, you may leverage the examples/pytorch/question-answering/run_squad.py script.\n",
        "\"\"\"\n",
        "query = \"What is a good example of a question answering dataset?\""
      ],
      "metadata": {
        "id": "2KUhb8P2s_mG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5 : Providing Data to Model**\n",
        "Providing **`context`** and **`query`** to the model function to model i.e. **`question_answerer()`**, which we created in **[Step 3 : Loading Model](https://colab.research.google.com/drive/1XwnBVcN7JnK6fcOpJard3LsNBOj1UnjC#scrollTo=zn4byRiUjxxz)**"
      ],
      "metadata": {
        "id": "scQrv8gUtSPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = question_answerer(question=query, context=context)"
      ],
      "metadata": {
        "id": "lqEvXyjSuuAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6 : Display Output**\n",
        "The data taken and processed by the model is just diplayed in this step. "
      ],
      "metadata": {
        "id": "2hfC8Os8u6jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "id": "TcFImcMQvT3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}